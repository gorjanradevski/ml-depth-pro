{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f076f61-a158-4802-94f7-1395d97c33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorjanradevski/miniconda3/envs/iristick/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import depth_pro\n",
    "\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755b86c6-49b6-4d94-955f-18dd290935d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorjanradevski/Projects/ml-depth-pro/src/depth_pro/depth_pro.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(config.checkpoint_uri, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load model and preprocessing transform\n",
    "model = depth_pro.create_model(device=device, precision=torch.half)\n",
    "model.eval()\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049f80a5-3d2f-454a-ac11-ddd26e015361",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def inference(model, image_path, device):\n",
    "    # Load and preprocess image\n",
    "    image, _, f_px = depth_pro.load_rgb(image_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Depth in [m] and Focal length in pixels\n",
    "        depth, focal_length_px = model(image, f_px_override=f_px)\n",
    "\n",
    "    return depth, focal_length_px\n",
    "\n",
    "# Load your RGB image\n",
    "def visualize_results(image_path, depth=None, depth_image_path=None):\n",
    "    if depth is None and depth_image_path is None:\n",
    "        raise ValueError(\"both the depth and the depth_image cannot be none!\")\n",
    "\n",
    "    if depth is not None and depth_image_path is not None:\n",
    "        raise ValueError(\"both the depth and the depth_image cannot be provided!\")\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if depth is not None:\n",
    "        # Convert depth map to numpy for visualization\n",
    "        depth_map_np = depth.cpu().numpy()\n",
    "    \n",
    "        # Normalize depth map for better visualization\n",
    "        depth_map_normalized = (depth_map_np - depth_map_np.min()) / (depth_map_np.max() - depth_map_np.min())\n",
    "    else:\n",
    "        depth_map_normalized = Image.open(depth_image_path)\n",
    "    \n",
    "    # Plot the images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Show the original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"RGB Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Show the depth map (use colormap for better visibility)\n",
    "    axes[1].imshow(depth_map_normalized, cmap=\"viridis\")\n",
    "    axes[1].set_title(\"Depth Map\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bbf0c9-bede-488c-9062-250effb5c408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'inference' took 8.5100 seconds\n",
      "Function 'inference' took 8.5221 seconds\n",
      "Function 'inference' took 8.4280 seconds\n",
      "Function 'inference' took 8.4504 seconds\n",
      "Function 'inference' took 8.9466 seconds\n",
      "Function 'inference' took 9.8759 seconds\n"
     ]
    }
   ],
   "source": [
    "image_names = [\"409238553452200_color.png\", \"409243305337100_color.png\", \"409547560576900_color.png\", \"409570843067100_color.png\", \n",
    "               \"409573045097900_color.png\", \"409592125011600_color.png\"]\n",
    "\n",
    "root_path = Path(\"data/saved_images/\")\n",
    "# image_path = \"data/saved_images/409238553452200_color.png\"\n",
    "for image_name in image_names:\n",
    "    image_path = root_path / image_name\n",
    "    image = Image.open(image_path)\n",
    "    depth, focallength_px = inference(model=model, image_path=image_path, device=device)\n",
    "    # depth = torch.rand(image.size)\n",
    "    depth_name = image_name.replace(\"_color\", \"_depthpro\")\n",
    "    tensor_path = Path(depth_name).with_suffix(\".pt\")\n",
    "    torch.save(depth, open(root_path / tensor_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b360f0-c33c-4193-94f5-b6dc8fa028cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(image_path=image_path, depth=depth)\n",
    "depth_image_path = \"data/saved_images/409238553452200_depth_scaled.png\"\n",
    "visualize_results(image_path=image_path, depth_image_path=depth_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54135c-9462-4141-8e28-92e99930928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = [(512, 512)]#, (1024, 1024), (1920, 1080), (2268, 3024)]\n",
    "num_runs = 10  # Number of repetitions per image size\n",
    "warmup_runs = 2\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "for size in image_sizes:\n",
    "    print(f\"Running inference for image size: {size}\")\n",
    "\n",
    "    # Resize and save a temporary image\n",
    "    image = Image.open(image_path).resize(size)\n",
    "    temp_image_path = \"temp_resized.jpg\"\n",
    "    image.save(temp_image_path)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    for nr in range(num_runs):\n",
    "        # Warmup runs not timed\n",
    "        if nr >= warmup_runs:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "        depth, focallength_px = inference(model=model, image_path=temp_image_path, device=device)\n",
    "\n",
    "        if nr >= warmup_runs:\n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time = (end_time - start_time) * 1000  # convert to ms\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "        # Sync device to ensure accurate timing\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        elif device.type == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "\n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "\n",
    "    print(f\"Inference time for {size}: {mean_time:.2f} ± {std_time:.2f} ms\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8b5fa-1503-4fd0-a8d6-513853d34b46",
   "metadata": {},
   "source": [
    "## Image size and performance\n",
    "\n",
    "| Image Size      | Inference Time (ms) | Std Dev (ms) |\n",
    "|---------------|-------------------|-------------|\n",
    "| (512, 512)   | 547.06            | ±2.49       |\n",
    "| (1024, 1024) | 593.40            | ±9.39       |\n",
    "| (1920, 1080) | 638.44            | ±6.34       |\n",
    "| (2268, 3024) | 699.48            | ±1.97       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4e3a3-f646-4b86-a02f-856ecbeaca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing images\n",
    "image_dir = \"data/hands\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "# Define grid size: each row contains (Image, Depth Map) pairs\n",
    "num_cols = 2\n",
    "num_rows = len(image_paths)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5 * num_rows))\n",
    "\n",
    "# Process and display each image with its depth map\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    depth, _ = inference(image_path)  # Perform inference\n",
    "    image = Image.open(image_path)  # Load image\n",
    "\n",
    "    depth_np = depth.cpu().numpy() if isinstance(depth, torch.Tensor) else depth\n",
    "    depth_min, depth_max = depth_np.min(), depth_np.max()\n",
    "    if depth_max > depth_min:  # Avoid division by zero\n",
    "        depth_np = (depth_np - depth_min) / (depth_max - depth_min)\n",
    "   \n",
    "\n",
    "    # Display original image\n",
    "    ax1 = axes[i, 0] if num_rows > 1 else axes[0]\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(f\"Image {i+1}\")\n",
    "\n",
    "    # Display normalized depth map\n",
    "    ax2 = axes[i, 1] if num_rows > 1 else axes[1]\n",
    "    ax2.imshow(depth_np, cmap=\"viridis\")  # \"viridis\" for better visibility\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(f\"Depth Map {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599fee7-0243-44b8-aea9-0b93c3fbf24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
