{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b607cc-68d2-4491-ae76-1a0efcf16fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from tabulate import tabulate\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e19336-4917-4a10-8dc0-1dd7b56b8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"The device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f446c-ddcc-4099-b30f-8a275d016244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "class DepthModel:\n",
    "    def __init__(self, model_name: str, device):\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    @timeit\n",
    "    def __call__(self, image: Image):\n",
    "        inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k:v.to(self.device) for k,v in inputs.items()}\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "    \n",
    "        # interpolate to original size\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            predicted_depth.cpu().unsqueeze(1),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        return prediction.cpu()\n",
    "\n",
    "model_name = \"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\"\n",
    "depth_model = DepthModel(model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbc0b2-a621-410b-800d-ed9fbb700c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/saved_images/real1_color.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a6695-9729-459f-a96d-a88c389ab2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = depth_model(image=image)\n",
    "\n",
    "# Create interactive image\n",
    "fig = px.imshow(depth.squeeze().numpy(), color_continuous_scale='viridis')\n",
    "\n",
    "# Customize hover to show depth value in meters\n",
    "fig.update_traces(\n",
    "    hovertemplate='x: %{x}<br>y: %{y}<br>depth: %{z:.3f} m<extra></extra>'\n",
    ")\n",
    "\n",
    "# Display it in notebook\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aed508-5845-4c24-b600-1afbc940772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rgb_and_depth(rgb: np.ndarray, gt_depth: np.ndarray, pred_depth: np.ndarray):\n",
    "    \"\"\"\n",
    "    Visualize an RGB image with corresponding ground truth and predicted depth maps.\n",
    "\n",
    "    Args:\n",
    "        rgb (np.ndarray): RGB image as a NumPy array (H, W, 3).\n",
    "        gt_depth (np.ndarray): Ground truth depth map in meters (H, W).\n",
    "        pred_depth (np.ndarray): Predicted depth map in meters (H, W).\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=[\"RGB Image\", \"GT Depth Map (m)\", \"Predicted Depth Map (m)\"],\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "\n",
    "    # RGB image (static)\n",
    "    fig.add_trace(\n",
    "        go.Image(z=rgb),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Ground Truth Depth Map\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=gt_depth,\n",
    "            colorscale='viridis',\n",
    "            colorbar=dict(title='Depth (m)'),\n",
    "            hovertemplate='x: %{x}<br>y: %{y}<br>depth: %{z:.3f} m<extra></extra>',\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Predicted Depth Map (hide second colorbar to avoid clutter)\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=pred_depth,\n",
    "            colorscale='viridis',\n",
    "            showscale=False,\n",
    "            hovertemplate='x: %{x}<br>y: %{y}<br>depth: %{z:.3f} m<extra></extra>',\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    # Flip y-axis for image coordinates\n",
    "    for i in range(1, 4):\n",
    "        fig.update_yaxes(autorange='reversed', row=1, col=i)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1400,\n",
    "        height=480,\n",
    "        margin=dict(t=40)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# PREDICTED DEPTH MAPTH FROM DEPTH PRO\n",
    "\n",
    "image_ids = [\"409238553452200\", \"409243305337100\", \"409547560576900\", \"409570843067100\", \"409573045097900\", \"409592125011600\"]\n",
    "\n",
    "for image_id in image_ids:\n",
    "    # --- Load depth map and convert to meters ---\n",
    "    depth_path = f\"../data/saved_images/{image_id}_depth.png\"\n",
    "    depth_mm = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "    gt_depth = depth_mm.astype(np.float32) / 1000.0  # mm to meters\n",
    "\n",
    "    # --- Load RGB image ---\n",
    "    rgb_path = f\"../data/saved_images/{image_id}_color.png\"\n",
    "    rgb = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Load depth map and convert to meters ---\n",
    "    depth_path = f\"../data/saved_images/{image_id}_depthpro.pt\"\n",
    "    pred_depth = torch.load(depth_path, weights_only=True).cpu().numpy()\n",
    "\n",
    "    visualize_rgb_and_depth(rgb=rgb, gt_depth=gt_depth, pred_depth=pred_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b563d-a23d-46d0-a600-8404ef84cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [\"409238553452200\", \"409243305337100\", \"409547560576900\", \"409570843067100\", \"409573045097900\", \"409592125011600\"]\n",
    "\n",
    "model_name = \"depth-anything/Depth-Anything-V2-Metric-Indoor-Small-hf\"\n",
    "depth_model = DepthModel(model_name=model_name, device=device)\n",
    "\n",
    "for image_id in image_ids:\n",
    "    # --- Load depth map and convert to meters ---\n",
    "    depth_path = f\"../data/saved_images/{image_id}_depth.png\"\n",
    "    depth_mm = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "    gt_depth = depth_mm.astype(np.float32) / 1000.0  # mm to meters\n",
    "\n",
    "    # --- Load RGB image ---\n",
    "    rgb_path = f\"../data/saved_images/{image_id}_color.png\"\n",
    "    rgb = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Load depth map and convert to meters ---\n",
    "    pred_depth = depth_model(image=Image.open(rgb_path)).squeeze().cpu().numpy()\n",
    "\n",
    "    visualize_rgb_and_depth(rgb=rgb, gt_depth=gt_depth, pred_depth=pred_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987931a6-6c9d-4f9d-a3c6-843371fde6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
